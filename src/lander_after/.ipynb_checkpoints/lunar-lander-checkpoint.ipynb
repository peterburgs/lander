{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\peter\\anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow_core\\contrib\\layers\\python\\layers\\layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "Loading agent weights from disk...\n",
      "INFO:tensorflow:Restoring parameters from ./weights/weights.h5\n",
      "Alpha: 0.0001 Gamma: 0.990 Epsilon 0.99973\n",
      "0, 194.52, 194.52, 0.00\n",
      "1, 196.77, 195.64, 0.00\n",
      "2, 116.07, 169.12, 0.00\n",
      "3, 192.05, 174.85, 0.00\n",
      "4, 95.00, 158.88, 0.00\n",
      "5, -16.74, 129.61, 0.00\n",
      "6, 233.38, 144.43, 0.00\n",
      "7, 183.63, 149.33, 0.00\n",
      "8, 265.88, 162.28, 0.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-80adc05eda20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    163\u001b[0m                                 \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m                         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m                         \u001b[0ms_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_terminal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-80adc05eda20>\u001b[0m in \u001b[0;36mchoose_action\u001b[1;34m(self, s)\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mchoose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m                         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Q\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTATE_SIZE\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNUMBER_OF_ACTIONS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-80adc05eda20>\u001b[0m in \u001b[0;36m_Q\u001b[1;34m(self, s)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Q\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQ_network\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQ_X\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Q_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1180\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "import numpy as np\n",
    "import random, tempfile, os\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "TRAINING = False\n",
    "\n",
    "LEARNING_RATE = [0.01, 0.001, 0.0001]\n",
    "DISCOUNT_FACTOR = [0.9, 0.99, 0.999]\n",
    "\n",
    "\"\"\"\n",
    "Here are the values of this constant in order to achieve a proper balance of exploitation versus exploration \n",
    "at 5,000 episodes:\n",
    "\n",
    "* 0.99910 - 99.99% exploitation + 0.01% exploration\n",
    "* 0.99941 - 99.95% exploitation + 0.05% exploration\n",
    "* 0.99954 - 99.90% exploitation + 0.10% exploration\n",
    "* 0.99973 - 99.75% exploitation + 0.25% exploration\n",
    "* 0.99987 - 99.50% exploitation + 0.50% exploration\n",
    "\"\"\"\n",
    "EPSILON_DECAY = [0.99910, 0.99941, 0.99954, 0.99973, 0.99987, 0.99999]\n",
    "\n",
    "LEARNING_EPISODES = 10000\n",
    "TESTING_EPISODES = 100\n",
    "REPLAY_BUFFER_SIZE = 500000\n",
    "REPLAY_BUFFER_BATCH_SIZE = 32\n",
    "MINIMUM_REWARD = -250\n",
    "STATE_SIZE = 8\n",
    "NUMBER_OF_ACTIONS = 4\n",
    "WEIGHTS_FILENAME = './weights/weights.h5'\n",
    "\n",
    "class Agent:\n",
    "\tdef __init__(self, training, learning_rate, discount_factor, epsilon_decay):\n",
    "\t\tself.training = training\n",
    "\t\tself.learning_rate = learning_rate\n",
    "\t\tself.discount_factor = discount_factor\n",
    "\t\tself.epsilon_decay = epsilon_decay\n",
    "\t\tself.epsilon = 1.0 if self.training else 0.0\n",
    "\t\tself.replay_buffer = deque(maxlen=REPLAY_BUFFER_SIZE)\n",
    "\n",
    "\t\tself._create_networks()\n",
    "\n",
    "\t\tself.saver = tf.train.Saver()\n",
    "\n",
    "\t\tself.sess = tf.Session()\n",
    "\t\tself.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\t\tif not training:\n",
    "\t\t\tself._load_weights()\n",
    "\n",
    "\tdef choose_action(self, s):\n",
    "\t\tif not self.training or np.random.rand() > self.epsilon:\n",
    "\t\t\treturn np.argmax(self._Q(np.reshape(s, [1, STATE_SIZE]))[0])\n",
    "\n",
    "\t\treturn np.random.choice(NUMBER_OF_ACTIONS)\n",
    "\n",
    "\tdef store(self, s, a, r, s_, is_terminal):\n",
    "\t\tif self.training:\n",
    "\t\t\tself.replay_buffer.append((np.reshape(s, [1, STATE_SIZE]), a, r, np.reshape(s_, [1, STATE_SIZE]), is_terminal))\n",
    "\n",
    "\tdef optimize(self, s, a, r, s_, is_terminal):\n",
    "\t\tif self.training and len(self.replay_buffer) > REPLAY_BUFFER_BATCH_SIZE:\n",
    "\t\t\tbatch = np.array(random.sample(list(self.replay_buffer), REPLAY_BUFFER_BATCH_SIZE))\n",
    "\t\t\ts = np.vstack(batch[:, 0])\n",
    "\t\t\ta = np.array(batch[:, 1], dtype=int)\n",
    "\t\t\tr = np.array(batch[:, 2], dtype=float)\n",
    "\t\t\ts_ = np.vstack(batch[:, 3])\n",
    "\n",
    "\t\t\tnon_terminal_states = np.where(batch[:, 4] == False)\n",
    "\n",
    "\t\t\tif len(non_terminal_states[0]) > 0:\n",
    "\t\t\t\ta_ = np.argmax(self._Q(s_)[non_terminal_states, :][0], axis=1)\n",
    "\t\t\t\tr[non_terminal_states] += np.multiply(self.discount_factor, self._Q_target(s_)[non_terminal_states, a_][0])\n",
    "\n",
    "\t\t\ty = self._Q(s)\n",
    "\t\t\ty[range(REPLAY_BUFFER_BATCH_SIZE), a] = r\n",
    "\t\t\tself._optimize(s, y)\n",
    "\n",
    "\tdef close(self):\n",
    "\t\tif self.training:\n",
    "\t\t\tprint(\"Saving agent weights to disk...\")\n",
    "\t\t\tsave_path = self.saver.save(self.sess, WEIGHTS_FILENAME)\n",
    "\n",
    "\tdef update(self): \n",
    "\t\tif self.training:\n",
    "\t\t\tQ_W1, Q_W2, Q_W3, Q_b1, Q_b2, Q_b3 = self._get_variables(\"Q\")\n",
    "\t\t\tQ_target_W1, Q_target_W2, Q_target_W3, Q_target_b1, Q_target_b2, Q_target_b3 = self._get_variables(\"Q_target\")\n",
    "\t\t\tself.sess.run([Q_target_W1.assign(Q_W1), Q_target_W2.assign(Q_W2), Q_target_W3.assign(Q_W3), Q_target_b1.assign(Q_b1), Q_target_b2.assign(Q_b2), Q_target_b3.assign(Q_b3)])\n",
    "\n",
    "\t\tif self.epsilon > 0.01:\n",
    "\t\t\tself.epsilon *= self.epsilon_decay\n",
    "\n",
    "\tdef _load_weights(self):\n",
    "\t\tprint(\"Loading agent weights from disk...\")\n",
    "\t\ttry:\n",
    "\t\t\tself.saver.restore(self.sess, WEIGHTS_FILENAME)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(\"Error loading agent weights from disk.\", e)\n",
    "\n",
    "\tdef _optimize(self, s, y):\n",
    "\t\toptimizer, loss, Q_network = self.sess.run([self.optimizer, self.loss, self.Q_network], {self.Q_X: s, self.Q_y: y})\n",
    "\n",
    "\tdef _Q(self, s):\n",
    "\t\treturn self.sess.run(self.Q_network, {self.Q_X: s})\n",
    "\n",
    "\tdef _Q_target(self, s):\n",
    "\t\treturn self.sess.run(self.Q_target_network, {self.Q_target_X: s})\n",
    "\n",
    "\tdef _create_networks(self):\n",
    "\t\twith tf.variable_scope(\"Q\", reuse=tf.AUTO_REUSE):\n",
    "\t\t\tself.Q_X, self.Q_network = self._create_network()\n",
    "\t\t\tself.Q_y = tf.placeholder(shape=[None, NUMBER_OF_ACTIONS], dtype=tf.float32, name=\"y\")\n",
    "\n",
    "\t\twith tf.name_scope(\"loss\"):\n",
    "\t\t\tself.loss = tf.reduce_mean(tf.squared_difference(self.Q_y, self.Q_network))\n",
    "\n",
    "\t\twith tf.name_scope(\"train\"):\n",
    "\t\t\tself.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "\n",
    "\t\twith tf.variable_scope(\"Q_target\"):\n",
    "\t\t\tself.Q_target_X, self.Q_target_network = self._create_network()\n",
    "\n",
    "\tdef _create_network(self):\n",
    "\t\tX = tf.placeholder(shape=[None, STATE_SIZE], dtype=tf.float32, name=\"X\")\n",
    "\n",
    "\t\tlayer1 = tf.contrib.layers.fully_connected(X, 32, activation_fn=tf.nn.relu)\n",
    "\t\tlayer2 = tf.contrib.layers.fully_connected(layer1, 32, activation_fn=tf.nn.relu)\n",
    "\t\tnetwork = tf.contrib.layers.fully_connected(layer2, NUMBER_OF_ACTIONS, activation_fn=None)\n",
    "\n",
    "\t\treturn X, network\n",
    "\n",
    "\tdef _get_variables(self, scope):\n",
    "\t\twith tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "\t\t\tW1 = tf.get_variable(\"fully_connected/weights\")\n",
    "\t\t\tW2 = tf.get_variable(\"fully_connected_1/weights\")\n",
    "\t\t\tW3 = tf.get_variable(\"fully_connected_2/weights\")\n",
    "\t\t\tb1 = tf.get_variable(\"fully_connected/biases\")\n",
    "\t\t\tb2 = tf.get_variable(\"fully_connected_1/biases\")\n",
    "\t\t\tb3 = tf.get_variable(\"fully_connected_2/biases\")\n",
    "\n",
    "\t\treturn W1, W2, W3, b1, b2, b3\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tnp.set_printoptions(precision=2)\n",
    "\n",
    "\tenv = gym.make(\"LunarLander-v2\")\n",
    "\taverage_reward = deque(maxlen=100)\n",
    "\n",
    "\tagent = Agent(TRAINING, LEARNING_RATE[2], DISCOUNT_FACTOR[1], EPSILON_DECAY[3])\n",
    "\n",
    "\tprint(\"Alpha: %.4f Gamma: %.3f Epsilon %.5f\" % (agent.learning_rate, agent.discount_factor, agent.epsilon_decay))\n",
    "\t\n",
    "\tfor episode in range(LEARNING_EPISODES if TRAINING else TESTING_EPISODES):\n",
    "\t\tcurrent_reward = 0\n",
    "\n",
    "\t\ts = env.reset()\n",
    "\n",
    "\t\tfor t in range(1000):\n",
    "\t\t\tif not TRAINING: \n",
    "\t\t\t\tenv.render()\n",
    "\n",
    "\t\t\ta = agent.choose_action(s)\n",
    "\t\t\ts_, r, is_terminal, info = env.step(a)\n",
    "\n",
    "\t\t\tcurrent_reward += r\n",
    "\n",
    "\t\t\tagent.store(s, a, r, s_, is_terminal)\n",
    "\t\t\tagent.optimize(s, a, r, s_, is_terminal)\n",
    "\n",
    "\t\t\ts = s_\n",
    "\n",
    "\t\t\tif is_terminal or current_reward < MINIMUM_REWARD:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\tagent.update()\n",
    "\n",
    "\t\taverage_reward.append(current_reward)\n",
    "\n",
    "\t\tprint(\"%i, %.2f, %.2f, %.2f\" % (episode, current_reward, np.average(average_reward), agent.epsilon))\n",
    "\n",
    "\tenv.close()\n",
    "\tagent.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
